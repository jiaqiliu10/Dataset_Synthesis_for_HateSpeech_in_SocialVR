{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:11.958490Z",
     "start_time": "2025-03-12T08:41:08.332223Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from data_splitting import X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab93817fed45a45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:12.017212Z",
     "start_time": "2025-03-12T08:41:11.982888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89658b08b2d836cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:14.341064Z",
     "start_time": "2025-03-12T08:41:12.214355Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "E:\\Python3.12\\Lib\\site-packages\\transformers\\modeling_utils.py:1435: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Load HateBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "num_labels = 2  # hate or none-hate\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c5f12e726482c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:14.368257Z",
     "start_time": "2025-03-12T08:41:14.365419Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train = pd.DataFrame(X_train)\n",
    "# X_val = pd.DataFrame(X_val)\n",
    "# X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b566597d9e106f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:14.398783Z",
     "start_time": "2025-03-12T08:41:14.392277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"GalaxyGamer: Hey, what's up? You ready to dive into this new game update? ShadowSlayer: Yeah, let's do it! Just need to blow off some steam after dealing with my annoying boss all day. You know how it is. GalaxyGamer: Totally get that. Work's been crazy for me too. But hey, at least we can escape here for a bit. By the way, did you catch that new sci-fi series everyone's talking about? ShadowSlayer: Nah, man. I don't waste my time on that garbage. It's just more brain-dead content for sheep. People eat that stuff up and think they're all cultured or something. GalaxyGamer: Oh, I mean, I thought it was pretty entertaining. But hey, each to their own, right? Anyway, let's see if we can conquer this raid today. ShadowSlayer: Yeah, let's crush it. Just hope we don't get stuck with any noobs. Can't stand when people act like they know what they're doing but just drag the team down. Like, just uninstall already. GalaxyGamer: Yeah, I hear you. We definitely need a good team to make it through. But everyone starts somewhere, right? We can always give them a hand if they need it. ShadowSlayer: Sure, whatever. As long as they don't waste my time. Anyway, let's jump in and show them how it's done.\",\n",
       "       \"Speaker 1: And they don't want to give Black people credit. They don't want to give us any credit. Speaker 2: They just want to blame everything on nigger. Speaker 1: Did some white man in Boston shoot his pregnant wife? And then shot himself, but they said nigger did it? Speaker 2: Always trying to blame some nigger. And they believed this shit. Speaker 1: Then the brother couldn't take it or something, and he could, I jumped off a fucking bridge, right? Speaker 2: Then they told, oh, trying to blame, god damn nigger. Speaker 1: That's why I'm gonna, I'm gonna start a, a, a new ad.\",\n",
       "       \"User1: Man, you're still as lame as ever. Your skills haven't improved a bit, have you? User2: Look who's talking! Have you even looked at your own score lately? You're at the bottom of the leaderboard, dude. User1: I may be at the bottom, but at least I don't cheat like some people. User2: Oh really? Is that your excuse for sucking at this game? User1: Says the one who can't play a game without using third-party software. User1: Wow, your avatar looks ridiculous. Did you even try to make it look good? User2: And yours look any better? It looks like you've been hit by a truck. User1: At least I didn't waste my time putting together a clown outfit. User2: Yeah, well, at least I don't look like a reject from a bad 80s movie. User1: Your gameplay is as mediocre as your sense of humor, you know that right? User2: Did anyone ever tell you that your insults are as lame as your strategies? User1: Did anyone ever tell you that you're as dumb as a bag of rocks? User2: Better than being a toxic low-life like you.\",\n",
       "       ...,\n",
       "       \"DragonSlayer: Hey, what's up? You ready to take on this next mission, or are you just gonna drag us down again like last time? PixelPanda: Haha, I'm ready! I've been practicing. But honestly, last time was rough. I think I've got the hang of it now, though. How's your day been? DragonSlayer: It's been alright, but I swear if you screw this up again, I'm gonna lose it. Like, do you even know what you're doing, or are you just pretending? PixelPanda: Chill, dude. I'm trying my best here. I think if we work together, we'll get through it. By the way, did you catch that new sci-fi series everyone's talking about? DragonSlayer: Nah, I don't bother with that crap. It's probably just more woke garbage, right? I can't stand how everything's gotta be so political these days. PixelPanda: Uh, I don't know about that. I thought it was pretty interesting. But yeah, I get it. To each their own, I guess. Let's focus on the mission, yeah? DragonSlayer: Whatever, Panda. Just don't screw it up this time.\",\n",
       "       \"Player1: Hey, Player2, how's it going? Player2: Not bad, just trying to figure out this new map. How about you? Player1: Pretty much the same here. It's a bit tricky, isn't it? Player2: Yeah, a lot trickier than I thought it'd be. But I like the challenge. ToxicUser: Oh, what a surprise! The two noobs struggling. Pathetic. Player1: Hey, we're all learning here, no need for the negativity. ToxicUser: Negativity? I'm just stating facts, you're both trash at this game. Player2: Alright, that's not cool, man. Let's keep the chat respectful. ToxicUser: Respectful? Or just as soft and sensitive as you? Player1: There's a difference between being sensitive and not tolerating unnecessary rudeness, ToxicUser. ToxicUser: Whatever. I don't have time to babysit you losers. Player2: We're here to have fun, not to put each other down. Remember that.\",\n",
       "       \"PersonA: I think you hate white people. PersonB: I am white. PersonA: Yes, but I think you hate yourself. PersonB: I do hate myself. PersonA: You have a lot of white guilt. PersonB: I don't. PersonA: Yes, I think you do. PersonB: I don't. PersonA: White people shouldn't have any guilt because white people are better.\"],\n",
       "      shape=(2075,), dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933db458c9534160",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:14.457384Z",
     "start_time": "2025-03-12T08:41:14.425181Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/merged_all_conversations.csv\")\n",
    "data = data[['message', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed96536abaef47c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:14.932390Z",
     "start_time": "2025-03-12T08:41:14.489255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2074\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 259\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 260\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 构造 DataFrame\n",
    "train_df = pd.DataFrame({\"text\": X_train, \"labels\": y_train}).dropna()\n",
    "val_df = pd.DataFrame({\"text\": X_val, \"labels\": y_val}).dropna()\n",
    "test_df = pd.DataFrame({\"text\": X_test, \"labels\": y_test}).dropna()\n",
    "\n",
    "# 转换为 Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e88a3c43925391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:14.977718Z",
     "start_time": "2025-03-12T08:41:14.966312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 9088, 16650, 2099, 1024, 4931, 1010, 2054, 1005, 1055, 2039, 1029, 2017, 3201, 2000, 11529, 2046, 2023, 2047, 2208, 10651, 1029, 6281, 24314, 1024, 3398, 1010, 2292, 1005, 1055, 2079, 2009, 999, 2074, 2342, 2000, 6271, 2125, 2070, 5492, 2044, 7149, 2007, 2026, 15703, 5795, 2035, 2154, 1012, 2017, 2113, 2129, 2009, 2003, 1012, 9088, 16650, 2099, 1024, 6135, 2131, 2008, 1012, 2147, 1005, 1055, 2042, 4689, 2005, 2033, 2205, 1012, 2021, 4931, 1010, 2012, 2560, 2057, 2064, 4019, 2182, 2005, 1037, 2978, 1012, 2011, 1996, 2126, 1010, 2106, 2017, 4608, 2008, 2047, 16596, 1011, 10882, 2186, 3071, 1005, 1055, 3331, 2055, 1029, 6281, 24314, 1024, 20976, 1010, 2158, 1012, 1045, 2123, 1005, 1056, 5949, 2026, 2051, 2006, 2008, 13044, 1012, 2009, 1005, 1055, 2074, 2062, 4167, 1011, 2757, 4180, 2005, 8351, 1012, 2111, 4521, 2008, 4933, 2039, 1998, 2228, 2027, 1005, 2128, 2035, 3226, 2094, 2030, 2242, 1012, 9088, 16650, 2099, 1024, 2821, 1010, 1045, 2812, 1010, 1045, 2245, 2009, 2001, 3492, 14036, 1012, 2021, 4931, 1010, 2169, 2000, 2037, 2219, 1010, 2157, 1029, 4312, 1010, 2292, 1005, 1055, 2156, 2065, 2057, 2064, 16152, 2023, 8118, 2651, 1012, 6281, 24314, 1024, 3398, 1010, 2292, 1005, 1055, 10188, 2009, 1012, 2074, 3246, 2057, 2123, 1005, 1056, 2131, 5881, 2007, 2151, 2053, 16429, 2015, 1012, 2064, 1005, 1056, 3233, 2043, 2111, 2552, 2066, 2027, 2113, 2054, 2027, 1005, 2128, 2725, 2021, 2074, 8011, 1996, 2136, 2091, 1012, 2066, 1010, 2074, 4895, 7076, 9080, 2140, 2525, 1012, 9088, 16650, 2099, 1024, 3398, 1010, 1045, 2963, 2017, 1012, 2057, 5791, 2342, 1037, 2204, 2136, 2000, 2191, 2009, 2083, 1012, 2021, 3071, 4627, 4873, 1010, 2157, 1029, 2057, 2064, 2467, 2507, 2068, 1037, 2192, 2065, 2027, 2342, 2009, 1012, 6281, 24314, 1024, 2469, 1010, 3649, 1012, 2004, 2146, 2004, 2027, 2123, 1005, 1056, 5949, 2026, 2051, 1012, 4312, 1010, 2292, 1005, 1055, 5376, 1999, 1998, 2265, 2068, 2129, 2009, 1005, 1055, 2589, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train'][0]['text'], truncation=True, padding=\"max_length\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14bd8ea648086ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:15.049830Z",
     "start_time": "2025-03-12T08:41:15.043300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 2074\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a245393c74a99a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:15.775447Z",
     "start_time": "2025-03-12T08:41:15.083165Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2074/2074 [00:00<00:00, 3996.49 examples/s]\n",
      "Map: 100%|██████████| 259/259 [00:00<00:00, 3601.29 examples/s]\n",
      "Map: 100%|██████████| 260/260 [00:00<00:00, 3660.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2074\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization text data\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# 批量处理数据\n",
    "train_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "val_dataset = dataset['validation'].map(tokenize_function, batched=True)\n",
    "test_dataset = dataset['test'].map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef8c3858ce068dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:41:15.805933Z",
     "start_time": "2025-03-12T08:41:15.797791Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns(['text'])\n",
    "val_dataset = val_dataset.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "785f2bcfcc774157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:43:36.179739Z",
     "start_time": "2025-03-12T08:43:36.175885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set format (Trainer requires PyTorch format)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "497ac91a59cef479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:49:48.281588Z",
     "start_time": "2025-03-12T08:49:48.278087Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 转换为 PyTorch dataset\n",
    "# def convert_to_torch(data):\n",
    "#     return {key: torch.tensor(val) for key, val in data.items() if key in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
    "#\n",
    "# train_dataset = list(map(convert_to_torch, train_dataset))\n",
    "# val_dataset = list(map(convert_to_torch, val_dataset))\n",
    "# test_dataset = list(map(convert_to_torch, test_dataset))\n",
    "\n",
    "\n",
    "# 创建 PyTorch DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f459877853cf6c9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:43:39.341675Z",
     "start_time": "2025-03-12T08:43:39.335952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d38a5c0f9b18c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:48:10.103620Z",
     "start_time": "2025-03-12T08:48:10.097107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': <class 'torch.Tensor'>, 'input_ids': <class 'torch.Tensor'>, 'attention_mask': <class 'torch.Tensor'>}\n"
     ]
    }
   ],
   "source": [
    "# check if type is tensor\n",
    "batch = next(iter(train_dataloader))\n",
    "print({k: type(v) for k, v in batch.items()})  # 查看每个字段的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2a3c92245806bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:49:52.990577Z",
     "start_time": "2025-03-12T08:49:52.644124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "Batch labels: tensor([1, 0, 1, 0, 1, 1, 1, 1])\n",
      "Unique labels: tensor([0, 1])\n",
      "Train label distribution: [ 940 1134]\n",
      "Val label distribution: [117 142]\n"
     ]
    }
   ],
   "source": [
    "# 检查一下\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch.keys())  # 确保 `labels` 存在\n",
    "    print(\"Batch labels:\", batch[\"labels\"])  # 打印部分 labels\n",
    "    print(\"Unique labels:\", torch.unique(batch[\"labels\"]))  # 查看标签的唯一值\n",
    "    break\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_labels = np.array([ex[\"labels\"] for ex in train_dataset])\n",
    "val_labels = np.array([ex[\"labels\"] for ex in val_dataset])\n",
    "\n",
    "print(\"Train label distribution:\", np.bincount(train_labels))\n",
    "print(\"Val label distribution:\", np.bincount(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfb98bc579314979",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:49:55.382211Z",
     "start_time": "2025-03-12T08:49:55.378536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2074\n",
      "259\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eebbdbcd7012afca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:51:14.279094Z",
     "start_time": "2025-03-12T08:51:14.274093Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置训练轮数\n",
    "epoch_num = 15\n",
    "\n",
    "# 设置优化器 (AdamW) 和学习率调度器\n",
    "# AdamW 是 transformers 推荐的优化器,线性学习率调度器可防止模型训练过快导致不稳定\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.01)\n",
    "num_training_steps = len(train_dataloader) * epoch_num  # 训练 3 轮\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259dd5ebb06bf6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-12T08:51:21.636385Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 18/260 [00:04<01:01,  3.90it/s]"
     ]
    }
   ],
   "source": [
    "# 微调模型的 classifier 层\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 初始化存储训练和验证准确率的列表\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "epochs = epoch_num  # 训练轮数\n",
    "\n",
    "# 训练 Loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    correct_train = 0\n",
    "    total_train = len(train_dataset)\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "        batch = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "\n",
    "        # print(batch['labels'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算训练集准确率\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct_train += (preds == batch[\"labels\"]).sum().item()\n",
    "\n",
    "        # total_train += batch[\"labels\"].size(0)\n",
    "\n",
    "    train_acc = correct_train / total_train\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # 计算验证集准确率\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = len(val_dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct_val += (preds == batch[\"labels\"]).sum().item()\n",
    "\n",
    "            # total_val += batch[\"labels\"].size(0)\n",
    "\n",
    "    val_acc = correct_val / total_val\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Acc: {train_acc:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# 绘制训练和验证准确率曲线\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, epochs+1), train_accuracies, label=\"Train Accuracy\", marker='o')\n",
    "plt.plot(range(1, epochs+1), val_accuracies, label=\"Validation Accuracy\", marker='s')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     loop = tqdm(train_dataloader, leave=True)\n",
    "#     for batch in loop:\n",
    "#         batch = {key: val.to(device) for key, val in batch.items()}  # 移动到 GPU/CPU\n",
    "#\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(**batch)  # 前向传播\n",
    "#         loss = outputs.loss  # 计算损失\n",
    "#         loss.backward()  # 反向传播\n",
    "#         optimizer.step()  # 更新参数\n",
    "#         lr_scheduler.step()  # 调整学习率\n",
    "#\n",
    "#         loop.set_description(f\"Epoch {epoch+1}\")\n",
    "#         loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24847347cd510bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
